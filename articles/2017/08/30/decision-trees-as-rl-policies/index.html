<html><head></head><body><article class="post-139 post type-post status-publish format-standard hentry category-uncategorized" id="post-139">
<header class="entry-header">
<h1 class="entry-title">Decision Trees as RL Policies</h1> </header><!-- .entry-header -->
<div class="entry-content">
<p>In supervised learning, there are very good “shallow” models like <a href="https://github.com/dmlc/xgboost">XGBoost</a> and <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a>. These models can learn powerful classifiers without an artificial neuron in sight. So why, then, is modern reinforcement learning totally dominated by neural networks? My answer: no good reason. And now I want to show everyone that <strong>shallow architectures can do RL too.</strong></p>
<p>Right now, using absolutely no feature engineering, I can train an ensemble of decision trees to play various video games from the raw pixels. The performance isn’t comparable to deep RL algorithms yet, and it may never be for vision-based tasks (for good reason!), but it’s fairly impressive nonetheless.</p>
<figure aria-describedby="caption-attachment-140" class="wp-caption aligncenter" id="attachment_140" style="width: 302px"><img alt="" class="wp-image-140 size-full" height="360" src="img/1-1-1-1-1-1-1-giphy.gif" width="302"/><figcaption class="wp-caption-text" id="caption-attachment-140">A tree ensemble playing Atari Pong</figcaption></figure>
<p>So how exactly do I train shallow models on RL tasks? You might have a few ideas, and <a href="https://github.com/unixpickle/treeagent/tree/2fccc17912ffa226a4717773d380f314913be5a7#branches">so did I</a>. Today, I’ll just be telling you about the one that actually worked.</p>
<p>The algorithm itself is so simple that I’m almost kind of embarrassed by my previous (failed) attempts at tree-based RL. Essentially, I use <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting</a> with gradients from a <a href="http://www.scholarpedia.org/article/Policy_gradient_methods">policy gradient estimator</a>. I call the resulting algorithm <em>policy gradient boosting</em>. In practice, I use a slightly more complex algorithm (a tree-based variant of <a href="https://arxiv.org/abs/1707.06347">PPO</a>), but there is probably plenty of room for simplification.</p>
<p>With policy gradient boosting, we build up an ensemble of trees in an additive fashion. For every batch of experience, we add a few more trees to our model, making minor adjustments in the direction of the policy gradient. After hundreds or even thousands of trees, we can end up with a pretty good policy.</p>
<p>Now that I’ve found an algorithm that works pretty well, I want to figure out better hyper-parameters for it. I doubt that tree-based PPO is the best (or even a good) technique, and I doubt that my regularization heuristic is very good either. Yet, even with these somewhat random choices, my models perform well on very difficult tasks! This has convinced me that shallow architectures could really disrupt the modern RL landscape, given the proper care and attention.</p>
<p>All the code for this project is in my <a href="https://github.com/unixpickle/treeagent">treeagent</a> repository, and there are some video demonstrations up on this <a href="https://www.youtube.com/playlist?list=PLK4ye0cIRuPlkhP1bUKvcnW8Nm6fiKX-z">YouTube playlist</a>. If you’re interested, feel free to contribute to treeagent on Github or send me a PM on Twitter.</p>
</div><!-- .entry-content -->
<!-- .entry-footer -->
</article></body></html>