<html><head></head><body><article class="post-177 post type-post status-publish format-standard hentry category-uncategorized" id="post-177">
<header class="entry-header">
<h1 class="entry-title">VQ-DRAW: A New Generative Model</h1> </header><!-- .entry-header -->
<div class="entry-content">
<p>Today I am extremely excited to release VQ-DRAW, a project which has been brewing for a few months now. I am really happy about my initial set of results, and I’m so pleased to be sharing this research with the world. The goal of this blog post is to describe, on a high-level, what VQ-DRAW does and how it works. For a more technical description, you can always check out the <a href="https://arxiv.org/abs/2003.01599">paper</a> or the <a href="https://github.com/unixpickle/vq-draw">code</a>.</p>
<p>In essence, VQ-DRAW is just a fancy compression algorithm. As it turns out, when a compression algorithm is good enough, decompressing random data produces realistic-looking results. For example, when VQ-DRAW is trained to compress pictures of faces into 75 bytes, this is what happens when it decompresses random data:</p>
<figure aria-describedby="caption-attachment-184" class="wp-caption aligncenter" id="attachment_184" style="width: 300px"><a href="img/1-1-1-1-1-1-1-samples-1-300x300.png"><img alt="" class="wp-image-184 size-medium" height="300" sizes="(max-width: 300px) 100vw, 300px" src="img/1-1-1-1-1-1-1-samples-1-300x300.png" width="300"/></a><figcaption class="wp-caption-text" id="caption-attachment-184">Samples from a VQ-DRAW model trained on the <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA dataset</a>.</figcaption></figure>
<p>Pretty cool, right? Of course, there’s plenty of other generative models out there that can dream up realistic looking images. VQ-DRAW has two properties that set it apart:</p>
<ol>
<li>The latent codes in VQ-DRAW are discrete, allowing direct application to lossy compression without introducing extra overhead for an encoding scheme. Of course, <a href="https://arxiv.org/abs/1711.00937">other</a> <a href="https://arxiv.org/abs/1611.01144">methods</a> can produce discrete latent codes as well, but most of these methods either don’t produce high-quality samples, or require additional layers of density modeling (e.g. <a href="https://arxiv.org/abs/1606.05328">PixelCNN</a>) on top of the latent codes.</li>
<li>VQ-DRAW encodes and decodes data in a sequential manner. This means that reconstruction quality degrades gracefully as latent codes are truncated. This could be used, for example, to implement progressive loading for images and other kinds of data.</li>
</ol>
<figure aria-describedby="caption-attachment-181" class="wp-caption aligncenter" id="attachment_181" style="width: 280px"><a href="img/1-1-1-1-1-1-1-mnist_stages.png"><img alt="" class="wp-image-181 size-full" height="280" loading="lazy" sizes="(max-width: 280px) 100vw, 280px" src="img/1-1-1-1-1-1-1-mnist_stages.png" width="280"/></a><figcaption class="wp-caption-text" id="caption-attachment-181">MNIST digits as more latent codes become available. The reconstructions gradually become crisper, which is ideal for progressive loading.</figcaption></figure>
<p>The core algorithm behind VQ-DRAW is actually quite simple. At each stage of encoding, a <strong>refinement network</strong> looks at the current reconstruction and proposes <em>K</em> variations to it. The variation with the best reconstruction error is chosen and passed along to the next stage. Thus, each stage refines the reconstruction by adding <em>log<sub>2</sub>(K)</em> bits of information to the latent code. When we run <em>N</em> stages of encoding, the latent code is thus <em>N*log<sub>2</sub>(K)</em> bits long.</p>
<figure aria-describedby="caption-attachment-180" class="wp-caption aligncenter" id="attachment_180" style="width: 210px"><a href="img/1-1-1-1-1-1-1-enc_example-210x300.png"><img alt="" class="wp-image-180 size-medium" height="300" loading="lazy" sizes="(max-width: 210px) 100vw, 210px" src="img/1-1-1-1-1-1-1-enc_example-210x300.png" width="210"/></a><figcaption class="wp-caption-text" id="caption-attachment-180">VQ-DRAW encoding an MNIST digit in stages. In this simplified example, K = 8 and N = 3.</figcaption></figure>
<p>To train the refinement network, we can simply minimize the L2 distance between images in the training set and their reconstructions. It might seem surprising that this works. After all, why should the network learn to produce useful variations at each stage? Well, as a side-effect of selecting the best refinement options for each input, the refinement network is influenced by a <a href="https://en.wikipedia.org/wiki/Vector_quantization">vector quantization</a> effect (the “VQ” in “<strong>VQ-</strong>DRAW”). This effect pulls different options towards different samples in the training set, causing a sort of clustering to take place. In the first stage, this literally means that the network clusters the training samples in a way similar to k-means.</p>
<p>With this simple idea and training procedure, we can train VQ-DRAW on any kind of data we want. However, I only experimented with image datasets for the paper (something I definitely plan to rectify in the future). Here are some image samples generated by VQ-DRAW:</p>
<figure aria-describedby="caption-attachment-185" class="wp-caption aligncenter" id="attachment_185" style="width: 660px"><a href="img/1-1-1-1-1-1-1-sample_screenshot-924x1024.png"><img alt="" class="wp-image-185 size-large" height="731" loading="lazy" sizes="(max-width: 660px) 100vw, 660px" src="img/1-1-1-1-1-1-1-sample_screenshot-924x1024.png" width="660"/></a><figcaption class="wp-caption-text" id="caption-attachment-185">Samples from VQ-DRAW models trained on four different datasets.</figcaption></figure>
<figure aria-describedby="caption-attachment-189" class="wp-caption aligncenter" id="attachment_189" style="width: 128px"><img alt="" class="size-full wp-image-189" height="128" loading="lazy" src="img/1-1-1-1-1-1-1-gradual-decode.gif" width="128"/><figcaption class="wp-caption-text" id="caption-attachment-189">CIFAR digits being decoded stage-by-stage. Each stage adds a few bits of information.</figcaption></figure>
<p>At the time of this release, there is still a lot of work left to be done on VQ-DRAW. I want to see how well the method scales to larger image datasets like ImageNet. I also want to explore various ways of improving VQ-DRAW’s modeling power, which could make it generate even better samples. Finally, I’d like to try VQ-DRAW on various kinds of data beyond images, such as text and audio. Wish me luck!</p>
</div><!-- .entry-content -->
<!-- .entry-footer -->
</article></body></html>